{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашняя работа №2 (часть 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом работы включу проверку PEP-8. Она закомментированна (перед этим я учла все ее замечания),\n",
    "потому что из-за нее все перестает работать как надо. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Лемматизирую и анализирую текст книги в Майстеме\n",
    "Анализирую текст, замеряю время, потраченное на анализ. Лемматизированный текст пригодится для дальнейших действий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem \n",
    "\n",
    "with open('tragedy.txt', 'r', encoding='utf-8') as f:\n",
    "    text = ' '.join(f.read().split('\\n'))\n",
    "\n",
    "m = Mystem()\n",
    "lemmas = m.lemmatize(text)\n",
    "lemma_text = ''.join(lemmas)\n",
    "\n",
    "%time analyzed_mystem = m.analyze(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняю результат анализа в файле mystem_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('mystem_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(analyzed_mystem, f, ensure_ascii = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизирую с помощью NLTK\n",
    "Также удаляю все символы, не являющиеся словами. Измеряю время.\n",
    "Токенизируется уже лемматизированный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.75 s\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "%time words = [w for w in word_tokenize(lemma_text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Анализирую с помощью Pymorphy\n",
    "Снова измеряю время."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "analyzed = []\n",
    "for word in words:\n",
    "    analyzed.append(morph.parse(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теперь подсчитаем проценты всех частей речи\n",
    "Похоже, что INFN = VERB, так как обрабатывались леммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN 63263\n",
      "ADJF 32878\n",
      "None 183\n",
      "CONJ 37136\n",
      "PREP 28052\n",
      "ADVB 21602\n",
      "PRCL 18926\n",
      "NUMR 1500\n",
      "INFN 49232\n",
      "NPRO 28415\n",
      "COMP 178\n",
      "VERB 592\n",
      "ADJS 588\n",
      "GRND 169\n",
      "PRTF 412\n",
      "PRED 1696\n",
      "INTJ 163\n",
      "NOUN 22.19871221292349 %. ADJF 11.536747548116567 %. None 0.06421390599505238 %. CONJ 13.030861273400355 %. PREP 9.843325087285296 %. ADVB 7.580048072705581 %. PRCL 6.6410512834008815 %. NUMR 0.5263434917627243 %. INFN 17.275295190974962 %. NPRO 9.970700212291876 %. COMP 0.06245942768917662 %. VERB 0.20773023141568855 %. ADJS 0.20632664877098797 %. GRND 0.05930136673860028 %. PRTF 0.14456901240416162 %. PRED 0.5951190413530536 %. INTJ 0.057195992771549374 %. "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pos = []\n",
    "for ana in analyzed:\n",
    "    pos.append(ana[0].tag.POS)\n",
    "\n",
    "count_pos = dict(Counter(pos))\n",
    "\n",
    "percent_dict = {}\n",
    "for part in list(count_pos):\n",
    "    percent = count_pos[part] / len(words) * 100\n",
    "    percent_dict[part] = percent\n",
    "    \n",
    "for key, value in percent_dict.items():\n",
    "    print(key, value, \"%\", end='. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Напишу функцию для составления частотного словаря и найду топ-20 глаголов и наречий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "быть: 4525, сказать: 1001, знать: 887, думать: 644, говорить: 635, хотеть: 598, понимать: 558, становиться: 535, видеть: 438, отвечать: 369\n",
      "теперь: 771, очень: 668, здесь: 565, уже: 556, потом: 486, тут: 418, тогда: 404, много: 384, хорошо: 375, где: 367\n"
     ]
    }
   ],
   "source": [
    "def search_pos(data, pos):\n",
    "    n_list = []\n",
    "    for ana in data:\n",
    "        if ana[0].tag.POS == pos:\n",
    "            n_list.append(ana[0].word)\n",
    "    return Counter(n_list).most_common(20)\n",
    "\n",
    "\n",
    "top_verbs = search_pos(analyzed, 'INFN')\n",
    "top_v = [x[0] + ': ' + str(x[1]) for x in top_verbs]\n",
    "print(', '.join(top_v))\n",
    "\n",
    "top_advs = search_pos(analyzed, 'ADVB')\n",
    "top_a = [x[0] + ': ' + str(x[1]) for x in top_advs]\n",
    "print(', '.join(top_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Составлю топ-25 биграмм\n",
    "Большинство из них представляет собой комбинацию из предлогов, частиц, личных местоимений, модальных глаголов, то есть наиболее частотных слов в языке. Также из этих данных можно предположить, чтл Клайд - главный герой книги. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "не мочь: 607. он не: 564. что он: 564. и не: 426. в этот: 408. мочь быть: 407. он быть: 371. то что: 343. и он: 341. я не: 341. не быть: 336. он и: 333. что она: 332. она не: 298. в тот: 293. у он: 289. с она: 286. она и: 265. и в: 259. с он: 257. это быть: 253. не знать: 253. клайд и: 252. и клайд: 249. он в: 247. "
     ]
    }
   ],
   "source": [
    "bigs = nltk.bigrams(words)\n",
    "top_bigrams = Counter(bigs).most_common(25)\n",
    "for pair in top_bigrams:\n",
    "    print(pair[0][0], pair[0][1] + ':', pair[1], end='. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Составлю топ-25 триграмм\n",
    "То же, что и с биграммами: частотные триграммы - комбинации из частотных слов языка. Похоже. что \"озеро большой выпь\", какое бы у него ни было первоначальное название, является популярным местом в книге."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в тот же : 129. в конец конец : 111. во всякий случай : 100. озеро большой выпь : 93. тот же время : 90. до сей пора : 89. он не мочь : 88. и все же : 85. по крайний мера : 82. о то что : 80. в самый дело : 77. белнеп и джефсон : 73. я не мочь : 68. о то как : 64. в этот минута : 63. после то как : 59. для то чтобы : 58. в этот время : 57. на озеро большой : 56. не знать что : 52. то что он : 51. в то что : 47. и в тот : 47. у он быть : 47. бухта третий миля : 45. "
     ]
    }
   ],
   "source": [
    "trigs = nltk.trigrams(words)\n",
    "top_trigrams = Counter(trigs).most_common(25)\n",
    "for pair in top_trigrams:\n",
    "    print(pair[0][0], pair[0][1], pair[0][2], ':', pair[1], end='. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Токенизирую и проанализирую оригинальный текст\n",
    "Делаю это для того, чтобы сохранить полученные данные в файле json. В условиях задания требуется сохранять начальную форму, что потеряло бы смысл, если бы я сохраняла анализ лемматизированных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "not_lemma_words = [w.lower() for w in word_tokenize(text) if w.isalpha()]\n",
    "\n",
    "not_lemma_analyzed = []\n",
    "for word in not_lemma_words:\n",
    "    not_lemma_analyzed.append(morph.parse(word))\n",
    "\n",
    "forms_data = []\n",
    "for ana in not_lemma_analyzed:\n",
    "    forms_data.append({ana[0].word:{'normal_form':ana[0].normal_form, 'Part of Speech':ana[0].tag.POS}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохраняю результат предыдущей обработки в файле pymorphy_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pymorphy_data.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(forms_data, f, ensure_ascii = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
